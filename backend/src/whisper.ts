import OpenAI from 'openai';
import fs from 'fs';
import path from 'path';
import { Transcript, TranscriptSegment } from './types';
import { AppError } from './errors';
import { downloadYouTubeAudio } from './audio-ytdlp';
import { getCachedTranscript, cacheTranscript } from './cache';

const TEMP_DIR = path.join(process.cwd(), 'temp');

// Ensure temp directory exists
if (!fs.existsSync(TEMP_DIR)) {
  fs.mkdirSync(TEMP_DIR, { recursive: true });
}


/**
 * Transcribe audio using OpenAI Whisper API
 */
export async function transcribeWithWhisper(
  audioPath: string,
  apiKey: string,
  language: string = 'pt'
): Promise<Transcript> {
  const client = new OpenAI({ apiKey });

  try {
    // Get file info
    const stats = fs.statSync(audioPath);
    const fileSizeMB = Math.round(stats.size / 1024 / 1024 * 10) / 10;
    console.log(`[Whisper] üìÅ Audio file size: ${fileSizeMB}MB`);
    
    // Estimate processing time (roughly 1 minute per 10MB)
    const estimatedTime = Math.ceil(fileSizeMB / 10);
    console.log(`[Whisper] ‚è±Ô∏è Estimated processing time: ${estimatedTime} minute(s)`);
    
    // Read audio file
    console.log(`[Whisper] üìñ Reading audio file...`);
    const audioFile = fs.createReadStream(audioPath);
    
    // Call Whisper API with verbose_json response format to get timestamps
    console.log(`[Whisper] üöÄ Sending to OpenAI Whisper API...`);
    console.log(`[Whisper] ‚åõ This may take ${estimatedTime} minute(s). Please wait...`);
    
    const startTime = Date.now();
    
    // Add timeout to prevent hanging (10 minutes for long videos)
    const timeoutMinutes = Math.max(10, estimatedTime * 2); // At least 10 min, or 2x estimated
    const controller = new AbortController();
    const timeoutId = setTimeout(() => {
      controller.abort();
      console.error(`[Whisper] ‚ùå Timeout after ${timeoutMinutes} minutes`);
    }, timeoutMinutes * 60 * 1000);
    
    console.log(`[Whisper] ‚è∞ Timeout set to ${timeoutMinutes} minutes`);
    
    try {
      const transcription = await client.audio.transcriptions.create({
        file: audioFile as any,
        model: 'whisper-1',
        language: language === 'pt' ? 'pt' : language,
        response_format: 'verbose_json',
        timestamp_granularities: ['segment'],
      });
      
      clearTimeout(timeoutId);
      
      const elapsedTime = Math.round((Date.now() - startTime) / 1000);
      console.log(`[Whisper] ‚úÖ Transcription completed in ${elapsedTime} seconds`);
    
      console.log(`[Whisper] üìä Received transcription response. Keys:`, Object.keys(transcription));
    console.log(`[Whisper] Has segments:`, 'segments' in transcription);
    console.log(`[Whisper] Segments count:`, (transcription as any).segments?.length || 0);

    // Parse the segments from Whisper response
    const segments: TranscriptSegment[] = [];
    
    if ('segments' in transcription && Array.isArray(transcription.segments)) {
      console.log(`[Whisper] Processing ${transcription.segments.length} segments...`);
      for (const segment of transcription.segments) {
        segments.push({
          text: segment.text,
          offset: segment.start,
          duration: segment.end - segment.start,
        });
      }
    } else if ('text' in transcription) {
      // Fallback if no segments are provided
      console.log(`[Whisper] No segments, using full text fallback`);
      const fullText = transcription.text as string;
      console.log(`[Whisper] Full text length: ${fullText.length} chars`);
      segments.push({
        text: fullText,
        offset: 0,
        duration: 0,
      });
    } else {
      console.error(`[Whisper] WARNING: No segments and no text in transcription!`);
    }
    
    console.log(`[Whisper] Final segments count: ${segments.length}`);

    const result: Transcript = {
      videoId: path.basename(audioPath, path.extname(audioPath)),
      language: transcription.language || language,
      segments,
      isAutoGenerated: false, // Whisper transcriptions are not auto-generated
    };
    
    console.log(`[Whisper] Returning transcript with ${segments.length} segments`);
    return result;
    } catch (apiError: any) {
      clearTimeout(timeoutId);
      
      if (apiError.name === 'AbortError') {
        throw new AppError({
          code: 'WHISPER_TIMEOUT' as any,
          message: `Whisper API timed out after ${timeoutMinutes} minutes`,
          userMessage: 'Transcri√ß√£o demorou muito tempo (timeout)',
          suggestions: [
            'Tente novamente',
            'O v√≠deo pode estar muito longo ou complexo',
            'Verifique sua conex√£o com a internet',
          ],
          httpStatus: 504,
        });
      }
      throw apiError;
    }
  } catch (error: any) {
    console.error(`[Whisper] ‚ùå Error during transcription:`, error.message);
    if (error.response?.status === 413) {
      throw new AppError({
        code: 'AUDIO_TOO_LARGE' as any,
        message: 'Audio file is too large for Whisper API',
        userMessage: 'Arquivo de √°udio muito grande (limite: 25MB)',
        suggestions: [
          'Tente com um v√≠deo mais curto',
          'O limite do Whisper API √© 25MB',
        ],
        httpStatus: 413,
      });
    }
    throw error;
  }
}

/**
 * Main function to get transcript using Whisper
 * This is called when YouTube subtitles are not available
 */
export async function getTranscriptWithWhisper(
  videoId: string,
  apiKey: string,
  language: string = 'pt'
): Promise<Transcript> {
  // Check cache first
  const cached = getCachedTranscript(videoId, language);
  if (cached) {
    console.log(`[Whisper] Using cached transcript for ${videoId}`);
    return cached.transcript;
  }

  let audioPath: string | null = null;
  
  try {
    // Step 1: Download audio from YouTube
    console.log(`\n[Whisper] üì• STEP 1/3: Downloading audio for video ${videoId}...`);
    audioPath = await downloadYouTubeAudio(videoId, 10800); // Max 3 hours for Whisper
    console.log(`[Whisper] ‚úÖ Audio downloaded successfully to ${audioPath}`);
    
    // Step 2: Transcribe with Whisper
    console.log(`\n[Whisper] üéôÔ∏è STEP 2/3: Transcribing audio with Whisper API...`);
    const transcript = await transcribeWithWhisper(audioPath, apiKey, language);
    
    // Step 3: Process and cache
    console.log(`\n[Whisper] üíæ STEP 3/3: Processing and caching transcript...`);
    
    // Update videoId to match the input
    transcript.videoId = videoId;
    
    // Cache the transcript
    cacheTranscript(videoId, language, transcript, 'whisper');
    
    console.log(`[Whisper] ‚úÖ All steps completed successfully!`);
    return transcript;
  } catch (error: any) {
    console.error(`[Whisper] ‚ùå Process failed:`, error.message);
    throw error;
  } finally {
    // Clean up temporary audio file
    if (audioPath && fs.existsSync(audioPath)) {
      fs.unlinkSync(audioPath);
    }
  }
}

/**
 * Alternative: Transcribe from direct URL (if supported)
 * Some services allow direct URL transcription
 */
export async function transcribeFromUrl(
  videoUrl: string,
  apiKey: string,
  language: string = 'pt'
): Promise<Transcript> {
  const client = new OpenAI({ apiKey });

  try {
    // Note: OpenAI Whisper doesn't directly support URLs
    // This is a placeholder for when/if they add this feature
    // Or you can use a service that downloads and processes
    
    throw new AppError({
      code: 'URL_TRANSCRIPTION_NOT_SUPPORTED' as any,
      message: 'Direct URL transcription not supported',
      userMessage: 'Transcri√ß√£o direta de URL n√£o suportada',
      suggestions: [
        'Use o m√©todo de download de √°udio',
        'Ou forne√ßa um arquivo de √°udio local',
      ],
      httpStatus: 501,
    });
  } catch (error) {
    throw error;
  }
}

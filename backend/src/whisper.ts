import OpenAI from 'openai';
import fs from 'fs';
import path from 'path';
import { Transcript, TranscriptSegment } from './types';
import { AppError } from './errors';
import { downloadYouTubeAudio } from './audio-ytdlp';
import { getCachedTranscript, cacheTranscript } from './cache';
import { splitAudioIntoChunks, cleanupChunks, mergeChunkTranscripts, type AudioChunk } from './audio-chunker';

const TEMP_DIR = path.join(process.cwd(), 'temp');

// Ensure temp directory exists
if (!fs.existsSync(TEMP_DIR)) {
  fs.mkdirSync(TEMP_DIR, { recursive: true });
}


/**
 * Transcribe audio using OpenAI Whisper API
 */
/**
 * Transcribe a single audio chunk using Whisper API
 */
async function transcribeChunk(
  chunkPath: string,
  apiKey: string,
  language: string,
  chunkIndex: number,
  totalChunks: number
): Promise<any> {
  const client = new OpenAI({ apiKey });
  
  console.log(`[Whisper] üéôÔ∏è Processing chunk ${chunkIndex + 1}/${totalChunks}...`);
  const audioFile = fs.createReadStream(chunkPath);
    
  const startTime = Date.now();
  
  try {
    const transcription = await client.audio.transcriptions.create({
      file: audioFile as any,
      model: 'whisper-1',
      language: language === 'pt' ? 'pt' : language,
      response_format: 'verbose_json',
      timestamp_granularities: ['segment'],
    });
    
    const elapsedTime = Math.round((Date.now() - startTime) / 1000);
    console.log(`[Whisper] ‚úÖ Chunk ${chunkIndex + 1} transcribed in ${elapsedTime} seconds`);
    
    return transcription;
  } catch (error: any) {
    console.error(`[Whisper] ‚ùå Error transcribing chunk ${chunkIndex + 1}:`, error.message);
    throw error;
  }
}

/**
 * Transcribe audio using OpenAI Whisper API with chunking support
 */
export async function transcribeWithWhisper(
  audioPath: string,
  apiKey: string,
  language: string = 'pt'
): Promise<Transcript> {
  try {
    // Get file info
    const stats = fs.statSync(audioPath);
    const fileSizeMB = Math.round(stats.size / 1024 / 1024 * 10) / 10;
    console.log(`[Whisper] üìÅ Audio file size: ${fileSizeMB}MB`);
    
    // Split audio into chunks if needed (24MB to stay under 25MB limit)
    const chunks = await splitAudioIntoChunks(audioPath, 24); // 24MB chunks
    
    if (chunks.length > 1) {
      console.log(`[Whisper] üîÑ Processing ${chunks.length} chunks...`);
    }
    
    // Process all chunks
    const chunkTranscripts: Array<{ segments: TranscriptSegment[], startOffset: number }> = [];
    
    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i];
      const transcription = await transcribeChunk(
        chunk.path,
        apiKey,
        language,
        i,
        chunks.length
      );
      
      console.log(`[Whisper] üìä Chunk ${i + 1} has ${transcription.segments?.length || 0} segments`);

      // Parse the segments from this chunk
      const chunkSegments: TranscriptSegment[] = [];
      
      if ('segments' in transcription && Array.isArray(transcription.segments)) {
        for (const segment of transcription.segments) {
          chunkSegments.push({
            text: segment.text,
            offset: segment.start,
            duration: segment.end - segment.start,
          });
        }
      } else if ('text' in transcription) {
        // Fallback if no segments
        chunkSegments.push({
          text: transcription.text as string,
          offset: 0,
          duration: chunk.duration,
        });
      }
      
      chunkTranscripts.push({
        segments: chunkSegments,
        startOffset: chunk.startTime
      });
    }
    
    // Merge all chunk transcripts
    const segments = chunks.length > 1 
      ? mergeChunkTranscripts(chunkTranscripts)
      : chunkTranscripts[0]?.segments || [];
    
    console.log(`[Whisper] üìä Total segments from all chunks: ${segments.length}`);
    
    // Clean up chunk files
    if (chunks.length > 1) {
      cleanupChunks(chunks);
    }

    const result: Transcript = {
      videoId: path.basename(audioPath, path.extname(audioPath)),
      language: language, // Use the provided language
      segments,
      isAutoGenerated: false, // Whisper transcriptions are not auto-generated
    };
    
    console.log(`[Whisper] Returning transcript with ${segments.length} segments`);
    return result;
  } catch (error: any) {
    console.error(`[Whisper] ‚ùå Error during transcription:`, error.message);
    if (error.response?.status === 413) {
      throw new AppError({
        code: 'AUDIO_TOO_LARGE' as any,
        message: 'Audio file is too large for Whisper API',
        userMessage: 'Arquivo de √°udio muito grande (limite: 25MB)',
        suggestions: [
          'Tente com um v√≠deo mais curto',
          'O limite do Whisper API √© 25MB',
        ],
        httpStatus: 413,
      });
    }
    throw error;
  }
}

/**
 * Main function to get transcript using Whisper
 * This is called when YouTube subtitles are not available
 */
export async function getTranscriptWithWhisper(
  videoId: string,
  apiKey: string,
  language: string = 'pt'
): Promise<Transcript> {
  // Check cache first
  const cached = getCachedTranscript(videoId, language);
  if (cached) {
    console.log(`[Whisper] Using cached transcript for ${videoId}`);
    return cached.transcript;
  }

  let audioPath: string | null = null;
  
  try {
    // Step 1: Download audio from YouTube
    console.log(`\n[Whisper] üì• STEP 1/3: Downloading audio for video ${videoId}...`);
    audioPath = await downloadYouTubeAudio(videoId, 10800); // Max 3 hours for Whisper
    console.log(`[Whisper] ‚úÖ Audio downloaded successfully to ${audioPath}`);
    
    // Step 2: Transcribe with Whisper
    console.log(`\n[Whisper] üéôÔ∏è STEP 2/3: Transcribing audio with Whisper API...`);
    const transcript = await transcribeWithWhisper(audioPath, apiKey, language);
    
    // Step 3: Process and cache
    console.log(`\n[Whisper] üíæ STEP 3/3: Processing and caching transcript...`);
    
    // Update videoId to match the input
    transcript.videoId = videoId;
    
    // Cache the transcript
    cacheTranscript(videoId, language, transcript, 'whisper');
    
    console.log(`[Whisper] ‚úÖ All steps completed successfully!`);
    return transcript;
  } catch (error: any) {
    console.error(`[Whisper] ‚ùå Process failed:`, error.message);
    throw error;
  } finally {
    // Clean up temporary audio file
    if (audioPath && fs.existsSync(audioPath)) {
      fs.unlinkSync(audioPath);
    }
  }
}

/**
 * Alternative: Transcribe from direct URL (if supported)
 * Some services allow direct URL transcription
 */
export async function transcribeFromUrl(
  videoUrl: string,
  apiKey: string,
  language: string = 'pt'
): Promise<Transcript> {
  const client = new OpenAI({ apiKey });

  try {
    // Note: OpenAI Whisper doesn't directly support URLs
    // This is a placeholder for when/if they add this feature
    // Or you can use a service that downloads and processes
    
    throw new AppError({
      code: 'URL_TRANSCRIPTION_NOT_SUPPORTED' as any,
      message: 'Direct URL transcription not supported',
      userMessage: 'Transcri√ß√£o direta de URL n√£o suportada',
      suggestions: [
        'Use o m√©todo de download de √°udio',
        'Ou forne√ßa um arquivo de √°udio local',
      ],
      httpStatus: 501,
    });
  } catch (error) {
    throw error;
  }
}

import { YoutubeTranscript } from 'youtube-transcript';
import type { Transcript, TranscriptSegment } from './types';
import { parseYouTubeError } from './errors';
import { getTranscriptWithWhisper } from './whisper';
import { getCachedTranscript, cacheTranscript } from './cache';
import { msToSeconds, formatTimestamp as formatTime } from './time-utils';

/**
 * Extract video ID from YouTube URL
 */
export function extractVideoId(url: string): string | null {
  const patterns = [
    /(?:youtube\.com\/watch\?v=|youtu\.be\/)([^&\n?#]+)/,
    /youtube\.com\/embed\/([^&\n?#]+)/,
    /youtube\.com\/v\/([^&\n?#]+)/,
  ];

  for (const pattern of patterns) {
    const match = url.match(pattern);
    if (match) {
      return match[1];
    }
  }

  return null;
}

/**
 * Get transcript from YouTube video
 * First tries to get existing subtitles, then falls back to Whisper speech-to-text
 */
/**
 * Get video metadata from YouTube
 */
export async function getVideoInfo(videoId: string): Promise<{ title: string; duration: number }> {
  try {
    const youtubedl = require('youtube-dl-exec');
    const info = await youtubedl(`https://www.youtube.com/watch?v=${videoId}`, {
      dumpJson: true,
      noCheckCertificates: true,
      noWarnings: true,
    });
    
    return {
      title: info.title || 'Unknown Video',
      duration: info.duration || 0,
    };
  } catch (error) {
    console.error(`[YouTube] Error fetching video info:`, error);
    return {
      title: 'Unknown Video',
      duration: 0,
    };
  }
}

export async function getTranscript(
  videoId: string,
  preferredLanguage: string = 'pt',
  useWhisperFallback: boolean = true,
  openaiApiKey?: string,
  forceWhisper: boolean = false
): Promise<{ transcript: Transcript; fromCache: boolean; videoTitle?: string }> {
  // If force Whisper, skip cache and subtitles
  if (forceWhisper && openaiApiKey) {
    console.log(`[YouTube] Force Whisper enabled, skipping subtitles for ${videoId}`);
    const whisperTranscript = await getTranscriptWithWhisper(
      videoId,
      openaiApiKey,
      preferredLanguage
    );
    whisperTranscript.isAutoGenerated = false;
    cacheTranscript(videoId, preferredLanguage, whisperTranscript, 'whisper');
    const videoInfo = await getVideoInfo(videoId);
    return { transcript: whisperTranscript, fromCache: false, videoTitle: videoInfo.title };
  }

  // Get video info first
  const videoInfo = await getVideoInfo(videoId);
  console.log(`[YouTube] Video title: "${videoInfo.title}"`);
  
  // Check cache
  const cached = getCachedTranscript(videoId, preferredLanguage);
  if (cached) {
    return { transcript: cached.transcript, fromCache: true, videoTitle: videoInfo.title };
  }

  try {
    // Try to fetch transcript with preferred language
    console.log(`[YouTube] Trying to fetch ${preferredLanguage} subtitles for ${videoId}...`);
    const transcriptItems = await YoutubeTranscript.fetchTranscript(videoId, {
      lang: preferredLanguage,
    });
    console.log(`[YouTube] Found ${transcriptItems.length} subtitle segments`);

    // Treat empty transcript as failure - trigger fallback
    if (!transcriptItems || transcriptItems.length === 0) {
      console.log(`[YouTube] No subtitle segments found for ${videoId} in ${preferredLanguage}, treating as no subtitles`);
      throw new Error('NO_SUBTITLE_SEGMENTS');
    }

    const segments: TranscriptSegment[] = transcriptItems.map((item) => ({
      text: item.text,
      offset: msToSeconds(item.offset),
      duration: msToSeconds(item.duration),
    }));

    const transcript: Transcript = {
      videoId,
      language: preferredLanguage,
      segments,
      isAutoGenerated: true, // youtube-transcript doesn't distinguish
    };

    // Cache the transcript
    cacheTranscript(videoId, preferredLanguage, transcript, 'youtube');

    return { transcript, fromCache: false, videoTitle: videoInfo.title };
  } catch (error) {
    // Try English as fallback
    if (preferredLanguage !== 'en') {
      try {
        console.log(`[YouTube] Trying English subtitles for ${videoId}...`);
        const transcriptItems = await YoutubeTranscript.fetchTranscript(videoId, {
          lang: 'en',
        });
        console.log(`[YouTube] Found ${transcriptItems.length} English subtitle segments`);

        // Treat empty transcript as failure - trigger fallback
        if (!transcriptItems || transcriptItems.length === 0) {
          console.log(`[YouTube] No English subtitle segments found for ${videoId}, treating as no subtitles`);
          throw new Error('NO_SUBTITLE_SEGMENTS');
        }

        const segments: TranscriptSegment[] = transcriptItems.map((item) => ({
          text: item.text,
          offset: msToSeconds(item.offset),
          duration: msToSeconds(item.duration),
        }));

        const transcript: Transcript = {
          videoId,
          language: 'en',
          segments,
          isAutoGenerated: true,
        };

        // Cache the transcript
        cacheTranscript(videoId, 'en', transcript, 'youtube');

        return { transcript, fromCache: false, videoTitle: videoInfo.title };
      } catch {
        // Fall through to error
      }
    }

    // Try without language specification
    try {
      console.log(`[YouTube] Trying auto-detect subtitles for ${videoId}...`);
      const transcriptItems = await YoutubeTranscript.fetchTranscript(videoId);
      console.log(`[YouTube] Found ${transcriptItems.length} auto-detected subtitle segments`);

      // Treat empty transcript as failure - trigger Whisper fallback
      if (!transcriptItems || transcriptItems.length === 0) {
        console.log(`[YouTube] No auto-detected subtitle segments found for ${videoId}, treating as no subtitles`);
        throw new Error('NO_SUBTITLE_SEGMENTS');
      }

      const segments: TranscriptSegment[] = transcriptItems.map((item) => ({
        text: item.text,
        offset: msToSeconds(item.offset),
        duration: msToSeconds(item.duration),
      }));

      const transcript: Transcript = {
        videoId,
        language: 'auto',
        segments,
        isAutoGenerated: true,
      };

      // Cache the transcript
      cacheTranscript(videoId, 'auto', transcript, 'youtube');

      return { transcript, fromCache: false, videoTitle: videoInfo.title };
    } catch (finalError) {
      // If Whisper fallback is enabled and we have an API key, try speech-to-text
      if (useWhisperFallback && openaiApiKey) {
        console.log(`[YouTube] No subtitles found for ${videoId}, using Whisper speech-to-text...`);
        try {
          const whisperTranscript = await getTranscriptWithWhisper(
            videoId,
            openaiApiKey,
            preferredLanguage
          );
          
          // Mark as speech-to-text generated
          whisperTranscript.isAutoGenerated = false;
          
          // Cache the Whisper transcript
          cacheTranscript(videoId, preferredLanguage, whisperTranscript, 'whisper');
          
          console.log(`[YouTube] Successfully transcribed with Whisper`);
          return { transcript: whisperTranscript, fromCache: false, videoTitle: videoInfo.title };
        } catch (whisperError: any) {
          console.error(`[YouTube] Whisper transcription failed:`, whisperError.message);
          // If Whisper also fails, throw the original error
          throw parseYouTubeError(finalError, videoId);
        }
      }
      
      // Parse and throw specific YouTube error
      throw parseYouTubeError(finalError, videoId);
    }
  }
}

/**
 * Format timestamp from seconds to MM:SS or HH:MM:SS.
 * Re-exported from time-utils for backward compatibility.
 * @see time-utils.formatTimestamp
 */
export function formatTimestamp(seconds: number): string {
  return formatTime(seconds);
}

/**
 * Format transcript for AI processing
 */
export function formatTranscriptForAI(segments: TranscriptSegment[]): string {
  return segments
    .map((segment) => `[${formatTimestamp(segment.offset)}] ${segment.text.trim()}`)
    .join('\n');
}

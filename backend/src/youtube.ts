import { YoutubeTranscript } from 'youtube-transcript';
import type { Transcript, TranscriptSegment } from './types';
import { parseYouTubeError } from './errors';
import { getTranscriptWithWhisper } from './whisper';
import { getCachedTranscript, cacheTranscript } from './cache';
import { msToSeconds, formatTimestamp as formatTime, calculateTotalDuration } from './time-utils';
import { VALIDATION_CONFIG } from './validation-config';

/**
 * Extract video ID from YouTube URL
 */
export function extractVideoId(url: string): string | null {
  const patterns = [
    /(?:youtube\.com\/watch\?v=|youtu\.be\/)([^&\n?#]+)/,
    /youtube\.com\/embed\/([^&\n?#]+)/,
    /youtube\.com\/v\/([^&\n?#]+)/,
  ];

  for (const pattern of patterns) {
    const match = url.match(pattern);
    if (match) {
      return match[1];
    }
  }

  return null;
}

/**
 * Get video metadata from YouTube
 */
export async function getVideoInfo(videoId: string): Promise<{ title: string; duration: number }> {
  try {
    const youtubedl = require('youtube-dl-exec');
    const info = await youtubedl(`https://www.youtube.com/watch?v=${videoId}`, {
      dumpJson: true,
      noCheckCertificates: true,
      noWarnings: true,
    });
    
    return {
      title: info.title || 'Unknown Video',
      duration: info.duration || 0,
    };
  } catch (error) {
    console.error(`[YouTube] Error fetching video info:`, error);
    return {
      title: 'Unknown Video',
      duration: 0,
    };
  }
}

/**
 * Calculate transcript coverage ratio.
 * Compares the duration covered by the transcript against the video duration.
 * 
 * @param segments - Transcript segments with timing information
 * @param videoDuration - Total video duration in seconds
 * @returns Coverage ratio (0.0 to 1.0), or 1.0 if video duration is unknown
 */
export function calculateTranscriptCoverage(
  segments: TranscriptSegment[],
  videoDuration: number
): number {
  if (videoDuration <= 0) {
    console.log(`[YouTube] Video duration unknown, assuming full coverage`);
    return 1.0;
  }

  if (segments.length === 0) {
    console.log(`[YouTube] No transcript segments, coverage is 0`);
    return 0.0;
  }

  const transcriptDuration = calculateTotalDuration(segments);
  const coverage = transcriptDuration / videoDuration;

  console.log(`[YouTube] Transcript coverage: ${(coverage * 100).toFixed(1)}% (${transcriptDuration.toFixed(1)}s / ${videoDuration.toFixed(1)}s)`);

  return coverage;
}

/**
 * Check if transcript coverage meets the required threshold.
 * 
 * @param segments - Transcript segments with timing information
 * @param videoDuration - Total video duration in seconds
 * @param threshold - Minimum coverage ratio required (default from config)
 * @returns true if coverage meets threshold, false otherwise
 */
export function checkTranscriptCoverage(
  segments: TranscriptSegment[],
  videoDuration: number,
  threshold: number = VALIDATION_CONFIG.TRANSCRIPT_COVERAGE_THRESHOLD
): boolean {
  const coverage = calculateTranscriptCoverage(segments, videoDuration);
  const meetsThreshold = coverage >= threshold;

  if (!meetsThreshold) {
    console.log(`[YouTube] Coverage ${(coverage * 100).toFixed(1)}% is below threshold ${(threshold * 100).toFixed(1)}%`);
  }

  return meetsThreshold;
}

/**
 * Get transcript from YouTube video
 * First tries to get existing subtitles, then falls back to Whisper speech-to-text
 * Also checks transcript coverage and falls back to Whisper if coverage is too low
 */
export async function getTranscript(
  videoId: string,
  preferredLanguage: string = 'pt',
  useWhisperFallback: boolean = true,
  openaiApiKey?: string,
  forceWhisper: boolean = false
): Promise<{ 
  transcript: Transcript; 
  fromCache: boolean; 
  videoTitle?: string;
  videoDuration?: number;
  transcriptCoverage?: number;
  whisperReason?: 'no_subtitles' | 'low_coverage' | 'force_whisper';
}> {
  // If force Whisper, skip cache and subtitles
  if (forceWhisper && openaiApiKey) {
    console.log(`[YouTube] Force Whisper enabled, skipping subtitles for ${videoId}`);
    const whisperTranscript = await getTranscriptWithWhisper(
      videoId,
      openaiApiKey,
      preferredLanguage
    );
    whisperTranscript.isAutoGenerated = false;
    cacheTranscript(videoId, preferredLanguage, whisperTranscript, 'whisper');
    const videoInfo = await getVideoInfo(videoId);
    const coverage = calculateTranscriptCoverage(whisperTranscript.segments, videoInfo.duration);
    return { 
      transcript: whisperTranscript, 
      fromCache: false, 
      videoTitle: videoInfo.title,
      videoDuration: videoInfo.duration,
      transcriptCoverage: coverage,
      whisperReason: 'force_whisper'
    };
  }

  // Get video info first
  const videoInfo = await getVideoInfo(videoId);
  console.log(`[YouTube] Video title: "${videoInfo.title}"`);
  
  // Check cache
  const cached = getCachedTranscript(videoId, preferredLanguage);
  if (cached) {
    const coverage = calculateTranscriptCoverage(cached.transcript.segments, videoInfo.duration);
    return { 
      transcript: cached.transcript, 
      fromCache: true, 
      videoTitle: videoInfo.title,
      videoDuration: videoInfo.duration,
      transcriptCoverage: coverage
    };
  }

  try {
    // Try to fetch transcript with preferred language
    console.log(`[YouTube] Trying to fetch ${preferredLanguage} subtitles for ${videoId}...`);
    const transcriptItems = await YoutubeTranscript.fetchTranscript(videoId, {
      lang: preferredLanguage,
    });
    console.log(`[YouTube] Found ${transcriptItems.length} subtitle segments`);

    // Treat empty transcript as failure - trigger fallback
    if (!transcriptItems || transcriptItems.length === 0) {
      console.log(`[YouTube] No subtitle segments found for ${videoId} in ${preferredLanguage}, treating as no subtitles`);
      throw new Error('NO_SUBTITLE_SEGMENTS');
    }

    const segments: TranscriptSegment[] = transcriptItems.map((item) => ({
      text: item.text,
      offset: msToSeconds(item.offset),
      duration: msToSeconds(item.duration),
    }));

    const transcript: Transcript = {
      videoId,
      language: preferredLanguage,
      segments,
      isAutoGenerated: true, // youtube-transcript doesn't distinguish
    };

    // Check transcript coverage - fall back to Whisper if coverage is too low
    if (useWhisperFallback && openaiApiKey && videoInfo.duration > 0) {
      const coverageMeetsThreshold = checkTranscriptCoverage(segments, videoInfo.duration);
      if (!coverageMeetsThreshold) {
        console.log(`[YouTube] Transcript coverage too low, falling back to Whisper for ${videoId}...`);
        try {
          const whisperTranscript = await getTranscriptWithWhisper(
            videoId,
            openaiApiKey,
            preferredLanguage
          );
          whisperTranscript.isAutoGenerated = false;
          cacheTranscript(videoId, preferredLanguage, whisperTranscript, 'whisper');
          console.log(`[YouTube] Successfully re-transcribed with Whisper due to low coverage`);
          const whisperCoverage = calculateTranscriptCoverage(whisperTranscript.segments, videoInfo.duration);
          return { 
            transcript: whisperTranscript, 
            fromCache: false, 
            videoTitle: videoInfo.title,
            videoDuration: videoInfo.duration,
            transcriptCoverage: whisperCoverage,
            whisperReason: 'low_coverage'
          };
        } catch (whisperError: any) {
          console.error(`[YouTube] Whisper fallback failed, using original transcript:`, whisperError.message);
          // Fall through to use the original transcript
        }
      }
    }

    // Cache the transcript
    cacheTranscript(videoId, preferredLanguage, transcript, 'youtube');

    const coverage = calculateTranscriptCoverage(segments, videoInfo.duration);
    return { 
      transcript, 
      fromCache: false, 
      videoTitle: videoInfo.title,
      videoDuration: videoInfo.duration,
      transcriptCoverage: coverage
    };
  } catch (error) {
    // Try English as fallback
    if (preferredLanguage !== 'en') {
      try {
        console.log(`[YouTube] Trying English subtitles for ${videoId}...`);
        const transcriptItems = await YoutubeTranscript.fetchTranscript(videoId, {
          lang: 'en',
        });
        console.log(`[YouTube] Found ${transcriptItems.length} English subtitle segments`);

        // Treat empty transcript as failure - trigger fallback
        if (!transcriptItems || transcriptItems.length === 0) {
          console.log(`[YouTube] No English subtitle segments found for ${videoId}, treating as no subtitles`);
          throw new Error('NO_SUBTITLE_SEGMENTS');
        }

        const segments: TranscriptSegment[] = transcriptItems.map((item) => ({
          text: item.text,
          offset: msToSeconds(item.offset),
          duration: msToSeconds(item.duration),
        }));

        const transcript: Transcript = {
          videoId,
          language: 'en',
          segments,
          isAutoGenerated: true,
        };

        // Check transcript coverage - fall back to Whisper if coverage is too low
        if (useWhisperFallback && openaiApiKey && videoInfo.duration > 0) {
          const coverageMeetsThreshold = checkTranscriptCoverage(segments, videoInfo.duration);
          if (!coverageMeetsThreshold) {
            console.log(`[YouTube] English transcript coverage too low, falling back to Whisper for ${videoId}...`);
            try {
              const whisperTranscript = await getTranscriptWithWhisper(
                videoId,
                openaiApiKey,
                preferredLanguage
              );
              whisperTranscript.isAutoGenerated = false;
              cacheTranscript(videoId, preferredLanguage, whisperTranscript, 'whisper');
              console.log(`[YouTube] Successfully re-transcribed with Whisper due to low English coverage`);
              const whisperCoverage = calculateTranscriptCoverage(whisperTranscript.segments, videoInfo.duration);
              return { 
                transcript: whisperTranscript, 
                fromCache: false, 
                videoTitle: videoInfo.title,
                videoDuration: videoInfo.duration,
                transcriptCoverage: whisperCoverage,
                whisperReason: 'low_coverage'
              };
            } catch (whisperError: any) {
              console.error(`[YouTube] Whisper fallback failed, using original English transcript:`, whisperError.message);
              // Fall through to use the original transcript
            }
          }
        }

        // Cache the transcript
        cacheTranscript(videoId, 'en', transcript, 'youtube');

        const coverage = calculateTranscriptCoverage(segments, videoInfo.duration);
        return { 
          transcript, 
          fromCache: false, 
          videoTitle: videoInfo.title,
          videoDuration: videoInfo.duration,
          transcriptCoverage: coverage
        };
      } catch {
        // Fall through to error
      }
    }

    // Try without language specification
    try {
      console.log(`[YouTube] Trying auto-detect subtitles for ${videoId}...`);
      const transcriptItems = await YoutubeTranscript.fetchTranscript(videoId);
      console.log(`[YouTube] Found ${transcriptItems.length} auto-detected subtitle segments`);

      // Treat empty transcript as failure - trigger Whisper fallback
      if (!transcriptItems || transcriptItems.length === 0) {
        console.log(`[YouTube] No auto-detected subtitle segments found for ${videoId}, treating as no subtitles`);
        throw new Error('NO_SUBTITLE_SEGMENTS');
      }

      const segments: TranscriptSegment[] = transcriptItems.map((item) => ({
        text: item.text,
        offset: msToSeconds(item.offset),
        duration: msToSeconds(item.duration),
      }));

      const transcript: Transcript = {
        videoId,
        language: 'auto',
        segments,
        isAutoGenerated: true,
      };

      // Check transcript coverage - fall back to Whisper if coverage is too low
      if (useWhisperFallback && openaiApiKey && videoInfo.duration > 0) {
        const coverageMeetsThreshold = checkTranscriptCoverage(segments, videoInfo.duration);
        if (!coverageMeetsThreshold) {
          console.log(`[YouTube] Auto-detected transcript coverage too low, falling back to Whisper for ${videoId}...`);
          try {
            const whisperTranscript = await getTranscriptWithWhisper(
              videoId,
              openaiApiKey,
              preferredLanguage
            );
            whisperTranscript.isAutoGenerated = false;
            cacheTranscript(videoId, preferredLanguage, whisperTranscript, 'whisper');
            console.log(`[YouTube] Successfully re-transcribed with Whisper due to low auto-detected coverage`);
            const whisperCoverage = calculateTranscriptCoverage(whisperTranscript.segments, videoInfo.duration);
            return { 
              transcript: whisperTranscript, 
              fromCache: false, 
              videoTitle: videoInfo.title,
              videoDuration: videoInfo.duration,
              transcriptCoverage: whisperCoverage,
              whisperReason: 'low_coverage'
            };
          } catch (whisperError: any) {
            console.error(`[YouTube] Whisper fallback failed, using original auto-detected transcript:`, whisperError.message);
            // Fall through to use the original transcript
          }
        }
      }

      // Cache the transcript
      cacheTranscript(videoId, 'auto', transcript, 'youtube');

      const coverage = calculateTranscriptCoverage(segments, videoInfo.duration);
      return { 
        transcript, 
        fromCache: false, 
        videoTitle: videoInfo.title,
        videoDuration: videoInfo.duration,
        transcriptCoverage: coverage
      };
    } catch (finalError) {
      // If Whisper fallback is enabled and we have an API key, try speech-to-text
      if (useWhisperFallback && openaiApiKey) {
        console.log(`[YouTube] No subtitles found for ${videoId}, using Whisper speech-to-text...`);
        try {
          const whisperTranscript = await getTranscriptWithWhisper(
            videoId,
            openaiApiKey,
            preferredLanguage
          );
          
          // Mark as speech-to-text generated
          whisperTranscript.isAutoGenerated = false;
          
          // Cache the Whisper transcript
          cacheTranscript(videoId, preferredLanguage, whisperTranscript, 'whisper');
          
          console.log(`[YouTube] Successfully transcribed with Whisper`);
          const whisperCoverage = calculateTranscriptCoverage(whisperTranscript.segments, videoInfo.duration);
          return { 
            transcript: whisperTranscript, 
            fromCache: false, 
            videoTitle: videoInfo.title,
            videoDuration: videoInfo.duration,
            transcriptCoverage: whisperCoverage,
            whisperReason: 'no_subtitles'
          };
        } catch (whisperError: any) {
          console.error(`[YouTube] Whisper transcription failed:`, whisperError.message);
          // If Whisper also fails, throw the original error
          throw parseYouTubeError(finalError, videoId);
        }
      }
      
      // Parse and throw specific YouTube error
      throw parseYouTubeError(finalError, videoId);
    }
  }
}

/**
 * Format timestamp from seconds to MM:SS or HH:MM:SS.
 * Re-exported from time-utils for backward compatibility.
 * @see time-utils.formatTimestamp
 */
export function formatTimestamp(seconds: number): string {
  return formatTime(seconds);
}

/**
 * Format transcript for AI processing
 */
export function formatTranscriptForAI(segments: TranscriptSegment[]): string {
  return segments
    .map((segment) => `[${formatTimestamp(segment.offset)}] ${segment.text.trim()}`)
    .join('\n');
}

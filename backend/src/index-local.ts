/**
 * Local YouTube Timestamp Generator API Server
 * 
 * This is the entry point for the fully local version of the application
 * that uses local Whisper and Ollama instead of OpenAI APIs.
 * 
 * No API keys required - everything runs locally!
 */

import express, { Request, Response, NextFunction } from 'express';
import cors from 'cors';
import dotenv from 'dotenv';
import { extractVideoId, getVideoInfo } from './youtube';
import { getTranscriptLocal } from './youtube-local';
import { generateTimestampsWithLocalLLM, getOllamaStatus } from './local-llm';
import { checkWhisperServerHealth, getWhisperServerInfo } from './local-whisper';
import { validateTimestamps } from './validator';
import { getCacheStats, clearOldCache, deleteCacheForVideo } from './cache';
import type { VideoRequest, TimestampResponse, GenerationMetadata } from './types';
import { AppError, createInvalidVideoIdError } from './errors';
import { VALIDATION_CONFIG } from './validation-config';

// Load environment variables
dotenv.config();

const app = express();
const PORT = process.env.PORT || 8000;

// Increase timeout for long videos (10 minutes)
const TIMEOUT_MS = 10 * 60 * 1000; // 10 minutes

// Middleware
app.use(cors());
app.use(express.json());

// Set timeout for all requests
app.use((req, res, next) => {
  req.setTimeout(TIMEOUT_MS);
  res.setTimeout(TIMEOUT_MS);
  next();
});

// Health check endpoints
app.get('/healthz', (_req: Request, res: Response) => {
  res.json({ status: 'ok', mode: 'local' });
});

app.get('/api/health', async (_req: Request, res: Response) => {
  // Check local services status
  const whisperHealthy = await checkWhisperServerHealth();
  const ollamaStatus = await getOllamaStatus();
  
  res.json({ 
    status: 'ok',
    mode: 'local',
    services: {
      whisper: {
        running: whisperHealthy,
        url: process.env.WHISPER_SERVER_URL || 'http://localhost:5000'
      },
      ollama: {
        running: ollamaStatus.running,
        model: ollamaStatus.currentModel,
        availableModels: ollamaStatus.models,
        url: process.env.OLLAMA_URL || 'http://localhost:11434'
      }
    }
  });
});

// Local services status endpoint
app.get('/api/services/status', async (_req: Request, res: Response) => {
  const whisperInfo = await getWhisperServerInfo();
  const ollamaStatus = await getOllamaStatus();
  
  res.json({
    whisper: whisperInfo || { status: 'offline', model: null, modelLoaded: false },
    ollama: ollamaStatus
  });
});

// Cache statistics endpoint
app.get('/api/cache/stats', (_req: Request, res: Response) => {
  const stats = getCacheStats();
  res.json(stats);
});

// Clear old cache endpoint
app.post('/api/cache/clear', (_req: Request, res: Response) => {
  clearOldCache();
  res.json({ message: 'Cache cleared successfully' });
});

// Delete cache for specific video
app.delete('/api/cache/:videoId', (req: Request, res: Response) => {
  const { videoId } = req.params;
  const { language = 'pt' } = req.query;
  const deleted = deleteCacheForVideo(videoId, language as string);
  res.json({ 
    message: deleted ? 'Cache deleted successfully' : 'Cache not found',
    deleted 
  });
});

// Endpoint to get transcript only (without AI timestamp generation)
app.post('/api/transcript', async (req: Request, res: Response, next: NextFunction) => {
  try {
    const { url, language = 'pt', force_whisper } = req.body as VideoRequest;

    // Validate request
    if (!url) {
      throw new AppError({
        code: 'INVALID_URL' as any,
        message: 'URL is required',
        userMessage: 'URL is required',
        suggestions: ['Provide a valid YouTube URL'],
        httpStatus: 400,
      });
    }

    // Extract video ID
    const videoId = extractVideoId(url);
    if (!videoId) {
      throw createInvalidVideoIdError();
    }

    // Get transcript using local services
    const { 
      transcript, 
      fromCache, 
      videoTitle, 
      videoDuration, 
      transcriptCoverage, 
      whisperReason 
    } = await getTranscriptLocal(videoId, language, true, force_whisper);

    // Return transcript with metadata
    res.json({
      transcript: {
        video_id: transcript.videoId,
        video_title: videoTitle,
        language: transcript.language,
        segments: transcript.segments,
        is_auto_generated: transcript.isAutoGenerated,
      },
      metadata: {
        video_id: videoId,
        video_title: videoTitle,
        language: transcript.language,
        is_auto_generated: transcript.isAutoGenerated,
        used_speech_to_text: !transcript.isAutoGenerated,
        whisper_reason: whisperReason,
        from_cache: fromCache,
        total_segments: transcript.segments.length,
        transcript_coverage: transcriptCoverage,
        video_duration: videoDuration,
        mode: 'local'
      },
    });
  } catch (error) {
    next(error);
  }
});

// Main endpoint to generate timestamps
app.post('/api/generate', async (req: Request, res: Response, next: NextFunction) => {
  try {
    const { url, language = 'pt', min_segment_duration = 30, force_whisper } = req.body as VideoRequest;

    // Validate request
    if (!url) {
      throw new AppError({
        code: 'INVALID_URL' as any,
        message: 'URL is required',
        userMessage: 'URL is required',
        suggestions: ['Provide a valid YouTube URL'],
        httpStatus: 400,
      });
    }

    // Extract video ID
    const videoId = extractVideoId(url);
    if (!videoId) {
      throw createInvalidVideoIdError();
    }

    // Get transcript using local services
    const { 
      transcript, 
      fromCache, 
      videoTitle, 
      videoDuration, 
      transcriptCoverage, 
      whisperReason 
    } = await getTranscriptLocal(videoId, language, true, force_whisper);

    // Generate timestamps with local LLM
    const aiResult = await generateTimestampsWithLocalLLM(
      transcript,
      min_segment_duration,
      videoTitle
    );

    // Validate timestamps using centralized config
    const validated = validateTimestamps(
      aiResult.timestamps || [],
      transcript,
      VALIDATION_CONFIG.DEFAULT_MIN_CONFIDENCE,
      min_segment_duration
    );

    // Build response with extended metadata
    const metadata: GenerationMetadata & { model_used?: string; mode?: string } = {
      video_id: videoId,
      language: transcript.language,
      is_auto_generated: transcript.isAutoGenerated,
      used_speech_to_text: !transcript.isAutoGenerated,
      whisper_reason: whisperReason,
      from_cache: fromCache,
      total_candidates: aiResult.timestamps?.length || 0,
      validated_count: validated.length,
      transcript_coverage: transcriptCoverage,
      video_duration: videoDuration,
      model_used: aiResult.modelUsed,
      mode: 'local'
    };

    const response: TimestampResponse = {
      timestamps: validated,
      metadata: metadata as GenerationMetadata,
    };

    res.json(response);
  } catch (error) {
    next(error);
  }
});

// Error handling middleware
app.use((err: Error | AppError, _req: Request, res: Response, _next: NextFunction) => {
  // Log error for debugging
  console.error('Error occurred:', {
    name: err.name,
    message: err.message,
    ...(err instanceof AppError ? { code: err.code, suggestions: err.suggestions } : {}),
  });

  // If it's our custom AppError, return structured error
  if (err instanceof AppError) {
    return res.status(err.httpStatus).json(err.toJSON());
  }

  // Generic error fallback
  res.status(500).json({
    error: {
      code: 'UNKNOWN_ERROR',
      message: 'Internal server error',
      suggestions: [
        'Try again later',
        'Make sure local services (Whisper, Ollama) are running',
        'Check the server logs for more details'
      ],
    },
  });
});

// Start server
app.listen(PORT, () => {
  console.log(`\n========================================`);
  console.log(`  Local YouTube Timestamp Generator`);
  console.log(`========================================`);
  console.log(`Server running on http://localhost:${PORT}`);
  console.log(`Mode: LOCAL (no API keys required)`);
  console.log(`\nRequired services:`);
  console.log(`  - Whisper: ${process.env.WHISPER_SERVER_URL || 'http://localhost:5000'}`);
  console.log(`  - Ollama:  ${process.env.OLLAMA_URL || 'http://localhost:11434'}`);
  console.log(`\nTo start Whisper server:`);
  console.log(`  cd backend/local-whisper && python server.py`);
  console.log(`\nTo start Ollama:`);
  console.log(`  ollama serve`);
  console.log(`  ollama pull ${process.env.OLLAMA_MODEL || 'llama3.2'}`);
  console.log(`========================================\n`);
});
